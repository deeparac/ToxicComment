{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import gensim\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Convolution1D, MaxPool1D, Flatten, BatchNormalization\n",
    "\n",
    "\n",
    "embedding_file = '../inputs/glove.6B.100d.txt'\n",
    "train_file = '../inputs/train.csv'\n",
    "test_file = '../inputs/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_file, index_col='id')\n",
    "df_test = pd.read_csv(test_file, index_col='id')\n",
    "# One test input is missing data, so we will just replace it by an empty string.\n",
    "df_test['comment_text'].fillna('', inplace=True)\n",
    "\n",
    "simple_tokens = df.comment_text.apply(gensim.utils.simple_preprocess)\n",
    "phrases = gensim.models.phrases.Phrases(simple_tokens)\n",
    "tokenizer = gensim.models.phrases.Phraser(phrases)\n",
    "tokenized_text = list(tokenizer[simple_tokens])\n",
    "corpus_dict = gensim.corpora.dictionary.Dictionary(tokenized_text)\n",
    "TARGET_CLASSES = df.columns[1:]\n",
    "targets = df[TARGET_CLASSES].values\n",
    "from sklearn.feature_extraction import stop_words\n",
    "stoplist = [x for x in stop_words.ENGLISH_STOP_WORDS]\n",
    "customlist = ['ll', 'd', 'm', 're']\n",
    "stoplist.extend(customlist)\n",
    "\n",
    "for sentence in tokenized_text:\n",
    "    for ind, word in enumerate(sentence):\n",
    "        if '_' in word:\n",
    "            seplist = word.split('_')\n",
    "            sentence.pop(ind)\n",
    "            pos = ind\n",
    "            for sep in seplist:\n",
    "                sentence.insert(pos, sep)\n",
    "                pos += 1\n",
    "\n",
    "clear_tokenized_text = []\n",
    "for sentence in tokenized_text:\n",
    "    clear = [x for x in sentence if not x in stoplist]\n",
    "    clear_tokenized_text.append(clear)\n",
    "    \n",
    "reversed_tokenized_text = []\n",
    "for sentence in tokenized_text:\n",
    "    reversed_tokenized_text.append(list(reversed(sentence)))\n",
    "word2vec = gensim.models.word2vec.Word2Vec(reversed_tokenized_text, window=5, size=300, min_count=2, workers=6)\n",
    "features = np.zeros((len(reversed_tokenized_text), word2vec.vector_size))\n",
    "for i, tokens in enumerate(reversed_tokenized_text):\n",
    "    tokens = [t for t in tokens if t in word2vec.wv.vocab]\n",
    "    if tokens:\n",
    "        features[i, :] = np.mean([word2vec.wv[t] / word2vec.wv.vocab[t].count for t in tokens], axis=0)\n",
    "\n",
    "MAX_SEQ_LEN = 150\n",
    "# Note: shifting indices by 1 as index 0 will be used for padding.\n",
    "docs = [[idx + 1 for idx in corpus_dict.doc2idx(doc)]  for doc in clear_tokenized_text]\n",
    "padded_docs = keras.preprocessing.sequence.pad_sequences(docs, maxlen=MAX_SEQ_LEN, truncating='post', value=0)\n",
    "max_idx = max(c for d in docs for c in d)\n",
    "embeddings = np.array([np.random.normal(size=word2vec.vector_size)]+ # for the '0' padding word\n",
    "                      [word2vec.wv[corpus_dict[idx]]\n",
    "                      if corpus_dict[idx] in word2vec.wv.vocab\n",
    "                      else np.random.normal(size=word2vec.vector_size)\n",
    "                      for idx in range(max_idx)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/10\n",
      "86265/86265 [==============================] - 7s 84us/step - loss: 0.0948 - acc: 0.9678 - val_loss: 0.1009 - val_acc: 0.9666\n",
      "Epoch 2/10\n",
      "86265/86265 [==============================] - 7s 76us/step - loss: 0.0839 - acc: 0.9699 - val_loss: 0.0864 - val_acc: 0.9689\n",
      "Epoch 3/10\n",
      "86265/86265 [==============================] - 7s 77us/step - loss: 0.0828 - acc: 0.9703 - val_loss: 0.0845 - val_acc: 0.9695\n",
      "Epoch 4/10\n",
      "86265/86265 [==============================] - 7s 79us/step - loss: 0.0816 - acc: 0.9706 - val_loss: 0.0824 - val_acc: 0.9703\n",
      "Epoch 5/10\n",
      "86265/86265 [==============================] - 7s 77us/step - loss: 0.0807 - acc: 0.9710 - val_loss: 0.0813 - val_acc: 0.9706\n",
      "Epoch 6/10\n",
      "86265/86265 [==============================] - 7s 76us/step - loss: 0.0799 - acc: 0.9713 - val_loss: 0.0855 - val_acc: 0.9683\n",
      "Epoch 7/10\n",
      "86265/86265 [==============================] - 7s 77us/step - loss: 0.0791 - acc: 0.9715 - val_loss: 0.0803 - val_acc: 0.9711\n",
      "Epoch 8/10\n",
      "86265/86265 [==============================] - 7s 79us/step - loss: 0.0784 - acc: 0.9719 - val_loss: 0.0844 - val_acc: 0.9688\n",
      "Epoch 9/10\n",
      "86265/86265 [==============================] - 7s 81us/step - loss: 0.0781 - acc: 0.9720 - val_loss: 0.0788 - val_acc: 0.9720\n",
      "Epoch 10/10\n",
      "86265/86265 [==============================] - 7s 80us/step - loss: 0.0773 - acc: 0.9722 - val_loss: 0.0786 - val_acc: 0.9719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feab12c1fd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = Sequential()\n",
    "nn.add(Dense(1024, activation='elu', input_shape=(word2vec.vector_size,)))\n",
    "nn.add(Dense(512, activation='elu'))\n",
    "nn.add(Dense(128, activation='elu'))\n",
    "nn.add(Dense(len(TARGET_CLASSES), activation='sigmoid'))\n",
    "nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn.fit(features, targets, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/5\n",
      "86265/86265 [==============================] - 26s 299us/step - loss: 0.0947 - acc: 0.9718 - val_loss: 0.0578 - val_acc: 0.9800\n",
      "Epoch 2/5\n",
      "86265/86265 [==============================] - 25s 289us/step - loss: 0.0488 - acc: 0.9826 - val_loss: 0.0560 - val_acc: 0.9810\n",
      "Epoch 3/5\n",
      "86265/86265 [==============================] - 25s 286us/step - loss: 0.0386 - acc: 0.9856 - val_loss: 0.0581 - val_acc: 0.9806\n",
      "Epoch 4/5\n",
      "86265/86265 [==============================] - 25s 292us/step - loss: 0.0328 - acc: 0.9874 - val_loss: 0.0627 - val_acc: 0.9804\n",
      "Epoch 5/5\n",
      "86265/86265 [==============================] - 25s 288us/step - loss: 0.0283 - acc: 0.9888 - val_loss: 0.0675 - val_acc: 0.9802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea8033ce48>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "\n",
    "num_filters = 64\n",
    "weight_decay = 1e-4\n",
    "\n",
    "cnn = Sequential()\n",
    "cnn.add(Embedding(max_idx + 1, word2vec.vector_size, input_length=MAX_SEQ_LEN))\n",
    "cnn.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "cnn.add(MaxPooling1D(2))\n",
    "cnn.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "cnn.add(GlobalMaxPooling1D())\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "cnn.add(Dense(len(TARGET_CLASSES), activation='sigmoid'))  #multi-label (k-hot encoding)\n",
    "\n",
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "cnn.fit(padded_docs, targets, batch_size=128, epochs=5, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 91058 samples, validate on 4793 samples\n",
      "Epoch 1/3\n",
      "91058/91058 [==============================] - 465s 5ms/step - loss: 0.0753 - acc: 0.9758 - val_loss: 0.0506 - val_acc: 0.9814\n",
      "Epoch 2/3\n",
      "91058/91058 [==============================] - 451s 5ms/step - loss: 0.0438 - acc: 0.9832 - val_loss: 0.0543 - val_acc: 0.9806\n",
      "Epoch 3/3\n",
      "91058/91058 [==============================] - 449s 5ms/step - loss: 0.0355 - acc: 0.9861 - val_loss: 0.0561 - val_acc: 0.9806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea50756f60>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = Sequential()\n",
    "rnn.add(Embedding(max_idx + 1, word2vec.vector_size, input_length=MAX_SEQ_LEN))\n",
    "rnn.add(Bidirectional(LSTM(256, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)))\n",
    "rnn.add(Bidirectional(LSTM(256, return_sequences=False, dropout=0.1, recurrent_dropout=0.1)))\n",
    "rnn.add(Dense(256, activation='elu'))\n",
    "rnn.add(Dropout(0.25))\n",
    "rnn.add(Dense(128, activation='elu'))\n",
    "rnn.add(Dropout(0.25))\n",
    "rnn.add(Dense(len(TARGET_CLASSES), activation='sigmoid'))\n",
    "\n",
    "rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "rnn.fit(padded_docs, targets, batch_size=128, epochs=3, validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(test_file, index_col='id')\n",
    "# One test input is missing data, so we will just replace it by an empty string.\n",
    "df['comment_text'].fillna('', inplace=True)\n",
    "\n",
    "simple_tokens = df.comment_text.apply(gensim.utils.simple_preprocess)\n",
    "phrases = gensim.models.phrases.Phrases(simple_tokens)\n",
    "tokenizer = gensim.models.phrases.Phraser(phrases)\n",
    "tokenized_text = list(tokenizer[simple_tokens])\n",
    "corpus_dict = gensim.corpora.dictionary.Dictionary(tokenized_text)\n",
    "TARGET_CLASSES = df.columns[1:]\n",
    "targets = df[TARGET_CLASSES].values\n",
    "from sklearn.feature_extraction import stop_words\n",
    "stoplist = [x for x in stop_words.ENGLISH_STOP_WORDS]\n",
    "customlist = ['ll', 'd', 'm', 're']\n",
    "stoplist.extend(customlist)\n",
    "\n",
    "for sentence in tokenized_text:\n",
    "    for ind, word in enumerate(sentence):\n",
    "        if '_' in word:\n",
    "            seplist = word.split('_')\n",
    "            sentence.pop(ind)\n",
    "            pos = ind\n",
    "            for sep in seplist:\n",
    "                sentence.insert(pos, sep)\n",
    "                pos += 1\n",
    "\n",
    "clear_tokenized_text = []\n",
    "for sentence in tokenized_text:\n",
    "    clear = [x for x in sentence if not x in stoplist]\n",
    "    clear_tokenized_text.append(clear)\n",
    "    \n",
    "reversed_tokenized_text = []\n",
    "for sentence in tokenized_text:\n",
    "    reversed_tokenized_text.append(list(reversed(sentence)))\n",
    "word2vec = gensim.models.word2vec.Word2Vec(reversed_tokenized_text, window=5, size=300, min_count=2, workers=6)\n",
    "features = np.zeros((len(reversed_tokenized_text), word2vec.vector_size))\n",
    "for i, tokens in enumerate(reversed_tokenized_text):\n",
    "    tokens = [t for t in tokens if t in word2vec.wv.vocab]\n",
    "    if tokens:\n",
    "        features[i, :] = np.mean([word2vec.wv[t] / word2vec.wv.vocab[t].count for t in tokens], axis=0)\n",
    "\n",
    "MAX_SEQ_LEN = 150\n",
    "# Note: shifting indices by 1 as index 0 will be used for padding.\n",
    "docs = [[idx + 1 for idx in corpus_dict.doc2idx(doc)]  for doc in clear_tokenized_text]\n",
    "padded_docs = keras.preprocessing.sequence.pad_sequences(docs, maxlen=MAX_SEQ_LEN, truncating='post', value=0)\n",
    "max_idx = max(c for d in docs for c in d)\n",
    "embeddings = np.array([np.random.normal(size=word2vec.vector_size)]+ # for the '0' padding word\n",
    "                      [word2vec.wv[corpus_dict[idx]]\n",
    "                      if corpus_dict[idx] in word2vec.wv.vocab\n",
    "                      else np.random.normal(size=word2vec.vector_size)\n",
    "                      for idx in range(max_idx)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226998/226998 [==============================] - 1s 3us/step\n",
      "226998/226998 [==============================] - 3s 15us/step\n",
      "226998/226998 [==============================] - 112s 492us/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'list_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-25aefb658c02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msample_submission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../inputs/sample_submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msample_submission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist_classes\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0msample_submission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../outputs/submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_classes' is not defined"
     ]
    }
   ],
   "source": [
    "y_nn = nn.predict(features, batch_size=1024, verbose=1)\n",
    "y_cnn = cnn.predict(padded_docs, batch_size=1024, verbose=1)\n",
    "y_rnn = rnn.predict(padded_docs, batch_size=1024, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = (4 * y_cnn + 6 * y_rnn) / 10\n",
    "\n",
    "df = pd.read_csv(train_file, index_col='id')\n",
    "TARGET_CLASSES = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "sample_submission = pd.read_csv('../inputs/sample_submission.csv')\n",
    "sample_submission[TARGET_CLASSES] = y_hat\n",
    "sample_submission.to_csv('../outputs/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
