{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharNetConfig(object):\n",
    "    def __init__(self, params=None):\n",
    "        if params is None:\n",
    "            self.conv_layers = [\n",
    "                    [256, 7, 3],\n",
    "                    [256, 7, 3],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, 3]\n",
    "                ]\n",
    "            self.fc_layers = [1024, 512, 10]\n",
    "            self.l0 = 128\n",
    "            self.alstr = 'abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:’’’/\\|_@#$%ˆ&* ̃‘+-=<>()[]{}]'\n",
    "            self.alphabet_size = len(self.alstr)\n",
    "        else:\n",
    "            self.conv_layers = params['conv_layers']\n",
    "            self.fc_layers = params['fc_layers']\n",
    "            self.l0 = params['l0']\n",
    "            slef.alstr = params['alstr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharNet(object):\n",
    "    \"\"\"docstring for CharNet.\"\"\"\n",
    "    def __init__(self, conv_layers,\n",
    "                        fc_layers,\n",
    "                        l0,\n",
    "                        alphabet_size,\n",
    "                        encoder,\n",
    "                        **args\n",
    "    ):\n",
    "        super(CharNet, self).__init__()\n",
    "        tf.set_random_seed(time.time())\n",
    "        self.l0 = l0\n",
    "        self.conv_layers = conv_layers\n",
    "        self.fc_layers = fc_layers\n",
    "        self.alphabet_size = alphabet_size\n",
    "\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        with tf.name_scope('Input'):\n",
    "            self.input_x = tf.placeholder(tf.int64, shape=[None, self.l0],\n",
    "                                          name='input_x')\n",
    "            self.input_y = tf.placeholder(tf.float32, shape=[None, 6],\n",
    "                                          name='input_y')\n",
    "            self.dropout_keep_prob = tf.placeholder(tf.float32,\n",
    "                                                    name='dropout_keep_prob')\n",
    "\n",
    "        with tf.name_scope('Embedding'):\n",
    "            x = tf.nn.embedding_lookup(encoder, self.input_x)\n",
    "            x = tf.expand_dims(x, -1)\n",
    "\n",
    "        # Configure conv layers\n",
    "        for i, layer_params in enumerate(conv_layers):\n",
    "            with tf.name_scope(\"Convolution\"):\n",
    "                filter_param = [\n",
    "                    layer_params[1],\n",
    "                    x.get_shape()[2].value, # l0\n",
    "                    x.get_shape()[3].value, # channels\n",
    "                    layer_params[0]\n",
    "                ]\n",
    "                W = tf.Variable(initializer(filter_param), dtype='float32', name='filter')\n",
    "\n",
    "                conv_layer = tf.nn.conv2d(x, W, [1, 1, 1, 1], 'VALID', name='conv')\n",
    "                conv_layer = tf.nn.relu(conv_layer, name='act_relu')\n",
    "\n",
    "            if not layer_params[-1] is None:\n",
    "                with tf.name_scope(\"MaxPooling\"):\n",
    "                    pool_layer = tf.nn.max_pool(conv_layer,\n",
    "                                            ksize=[1, layer_params[-1], 1, 1],\n",
    "                                            strides=[1, layer_params[-1], 1, 1],\n",
    "                                            padding='VALID')\n",
    "                    x = tf.transpose(pool_layer, [0, 1, 3, 2])\n",
    "            else:\n",
    "                x = tf.transpose(conv_layer, [0, 1, 3, 2])\n",
    "\n",
    "        # flatten conv output for fc\n",
    "        with tf.name_scope(\"Flatten\"):\n",
    "            x = tf.contrib.layers.flatten(x)\n",
    "\n",
    "        # Configure fc layers\n",
    "        for i, layer_units in enumerate(fc_layers):\n",
    "            with tf.name_scope(\"FullyConnected\"):\n",
    "                W = tf.Variable(initializer([x.get_shape()[-1].value, layer_units]),\n",
    "                                dtype='float32', name='W')\n",
    "                b = tf.Variable(initializer([layer_units]),\n",
    "                                dtype='float32', name='W')\n",
    "                x = tf.nn.xw_plus_b(x, W, b, name='fully-connected')\n",
    "                x = tf.nn.relu(x)\n",
    "\n",
    "            with tf.name_scope(\"Dropout\"):\n",
    "                x = tf.nn.dropout(x, self.dropout_keep_prob)\n",
    "\n",
    "        with tf.name_scope(\"Output\"):\n",
    "            W = tf.Variable(initializer([x.get_shape()[-1].value, 6]),\n",
    "                            dtype='float32', name='W')\n",
    "            b = tf.Variable(initializer([6]),\n",
    "                            dtype='float32', name='W')\n",
    "            self.yhat = tf.nn.sigmoid(tf.matmul(x, W) + b, name='output')\n",
    "\n",
    "        with tf.name_scope(\"Loss\"):\n",
    "            self.loss = tf.losses.log_loss(self.input_y, self.yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "    def __init__(self, file_path, alstr, l0, is_dev=False, batch_size=128, **args):\n",
    "        self.alstr = alstr\n",
    "        self.l0 = l0\n",
    "        self.is_dev = is_dev\n",
    "        self.batch_size = batch_size\n",
    "        self.raw_data = pd.read_csv(file_path)\n",
    "\n",
    "        self.alphabet = self.make_alphabet(self.alstr)\n",
    "        self.encoder, self.e_dict = self.one_hot_encoder(self.alphabet)\n",
    "        self.alphabet_size = len(self.alphabet)\n",
    "    \n",
    "        self.input_x = self.process_full_description(self.raw_data)\n",
    "        if not self.is_dev:\n",
    "            self.y = self.generate_y(self.raw_data)\n",
    "            \n",
    "    def shuffling(self):\n",
    "        shuffle_indices = np.random.permutation(np.arange(len(self.input_x)))\n",
    "        self.input_x = self.input_x[shuffle_indices]\n",
    "        self.y = self.y[shuffle_indices]\n",
    "\n",
    "    def next_batch(self, batch_num):\n",
    "        data_size = len(self.input_x)\n",
    "        start = batch_num * self.batch_size\n",
    "        end = min((batch_num + 1) * self.batch_size, data_size)\n",
    "        batch_x = self.input_x[start:end]\n",
    "        if self.is_dev == False:\n",
    "            batch_y = self.y[start:end]\n",
    "        else:\n",
    "            batch_y = None\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def process_full_description(self, df):\n",
    "        df['comment_text'] = df['comment_text'].astype('str')\n",
    "        df['desc_vecs'] = df['comment_text'].apply(\n",
    "                lambda x: self.doc_process(x, self.e_dict)\n",
    "        )\n",
    "        return df['desc_vecs'].values\n",
    "\n",
    "    def generate_y(self, df):\n",
    "        list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "        y = df[list_classes].values\n",
    "        return y\n",
    "\n",
    "    def one_hot_encoder(self, alphabet):\n",
    "        encoder_dict = {}\n",
    "        encoder = []\n",
    "\n",
    "        encoder_dict['UNK'] = 0\n",
    "        encoder.append(np.zeros(len(alphabet), dtype='float32'))\n",
    "\n",
    "        for i, alpha in enumerate(alphabet):\n",
    "            onehot = np.zeros(len(alphabet), dtype='float32')\n",
    "            encoder_dict[alpha] = i + 1\n",
    "            onehot[i] = 1\n",
    "            encoder.append(onehot)\n",
    "\n",
    "        encoder = np.array(encoder, dtype='float32')\n",
    "        return encoder, encoder_dict\n",
    "\n",
    "    def doc_process(self, desc, e_dict):\n",
    "        l = self.l0\n",
    "        desc = desc.strip().lower()\n",
    "        min_len = min(l, len(desc))\n",
    "        doc_vec = np.zeros(l, dtype='int64')\n",
    "        for j in range(min_len):\n",
    "            if desc[j] in e_dict:\n",
    "                doc_vec[j] = e_dict[desc[j]]\n",
    "            else:\n",
    "                doc_vec[j] = e_dict['UNK']\n",
    "        return doc_vec\n",
    "\n",
    "    def make_alphabet(self, alstr):\n",
    "        return [char for char in alstr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../inputs/train.csv'\n",
    "dev_file = '../inputs/test.csv'\n",
    "config = CharNetConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Data(train_file, config.alstr, config.l0, is_dev=False, batch_size=128)\n",
    "dev_data = Data(dev_file, config.alstr, config.l0, is_dev=True, batch_size=128)\n",
    "conf = tf.ConfigProto()\n",
    "conf.gpu_options.allow_growth=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(config=conf) as sess:\n",
    "    charnet = CharNet(config.conv_layers, config.fc_layers, config.l0, config.alphabet_size, train_data.encoder)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    grads = optimizer.compute_gradients(charnet.loss)\n",
    "    train_op = optimizer.apply_gradients(grads)\n",
    "\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", charnet.loss)\n",
    "\n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "    # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def train_step(x_batch, y_batch, step):\n",
    "        \"\"\"\n",
    "        A single training step\n",
    "        \"\"\"\n",
    "        y_batch = np.reshape(y_batch, (-1, 6))\n",
    "        mm = x_batch.tolist()\n",
    "        x_batch = np.array([k.tolist() for k in mm])\n",
    "\n",
    "        feed_dict = {\n",
    "            charnet.input_x: x_batch,\n",
    "            charnet.input_y: y_batch,\n",
    "            charnet.dropout_keep_prob: .5\n",
    "        }\n",
    "\n",
    "        _, summaries, loss = sess.run(\n",
    "            [train_op,\n",
    "             train_summary_op,\n",
    "             charnet.loss],\n",
    "            feed_dict\n",
    "        )\n",
    "\n",
    "        print(\"step {}, loss {:g}\".format(step, loss))\n",
    "        train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "    def predict_on_test(x_batch, results):\n",
    "        mm = x_batch.tolist()\n",
    "        x_batch = np.array([k.tolist() for k in mm])\n",
    "\n",
    "        feed_dict = {\n",
    "            charnet.input_x: x_batch,\n",
    "            charnet.dropout_keep_prob: 1.0\n",
    "        }\n",
    "        result = sess.run([charnet.yhat], feed_dict)\n",
    "        results.append(result)\n",
    "\n",
    "    for epoch in range(50):\n",
    "        print(\"epoch is: {}\".format(epoch))\n",
    "        train_data.shuffling()\n",
    "        for i in range(int(len(train_data.y)/train_data.batch_size) + 1):\n",
    "            input_x, y = train_data.next_batch(i)\n",
    "            train_step(input_x, y, i)\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                path = saver.save(sess, './model.ckpt')\n",
    "                print(\"Epoch {}, Saved model checkpoint to {}\\n\".format(epoch, path))\n",
    "    \n",
    "    results = []\n",
    "    submission = dev_data.raw_data['id']\n",
    "    for i in range(int(len(dev_data.input_x)/dev_data.batch_size) + 1):\n",
    "        input_x, _ = dev_data.next_batch(i)\n",
    "        predict_on_test(input_x, results)\n",
    "    \n",
    "    preds = []\n",
    "    for result in results:\n",
    "        for elem in result[0]:\n",
    "            preds.append(elem)\n",
    "\n",
    "    preds = np.array(preds)\n",
    "#     list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "#     submission = pd.DataFrame(preds, columns=list_classes)\n",
    "#     submission['id'] = dev_data.raw_data['id']\n",
    "#     cols = submission.columns.tolist()\n",
    "#     cols = cols[-1:] + cols[:-1]\n",
    "#     submission = submission[cols]\n",
    "#     submission.to_csv('../outputs/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "path = '../input/'\n",
    "comp = 'jigsaw-toxic-comment-classification-challenge/'\n",
    "EMBEDDING_FILE = '../inputs/glove.6B.100d.txt'\n",
    "TRAIN_DATA_FILE = '../inputs/train.csv'\n",
    "TEST_DATA_FILE = '../inputs/test.csv'\n",
    "\n",
    "embed_size = 100 # how big is each word vector\n",
    "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a comment to use\n",
    "\n",
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(LSTM(100, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x) \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_t, y, batch_size=512, epochs=5)\n",
    "y_test = model.predict([X_te], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../inputs/sample_submission.csv')\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv('../outputs/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
